name: Automl deploy nlp model
inputs:
- {name: gcp_project_id, type: String}
- {name: gcp_region, type: String}
- {name: model_display_name, type: String}
- {name: model_id}
- {name: api_endpoint, type: String, optional: true}
outputs:
- {name: model_display_name, type: String}
- {name: model_id, type: String}
- {name: status, type: String}
implementation:
  container:
    image: python:3.7
    command:
    - python3
    - -u
    - -c
    - |
      def automl_deploy_nlp_model(
        gcp_project_id,
        gcp_region,
        model_display_name,
        model_id,
        api_endpoint = None,
      ):
        import subprocess
        import sys
        subprocess.run([sys.executable, '-m', 'pip', 'install', 'googleapis-common-protos==1.6.0', '--no-warn-script-location'],
            env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'}, check=True)
        subprocess.run([sys.executable, '-m', 'pip', 'install', 'google-cloud-automl==0.9.0', '--quiet', '--no-warn-script-location'],
            env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'}, check=True)

        import google
        import logging
        from google.api_core.client_options import ClientOptions
        from google.api_core import exceptions
        from google.cloud import  automl
        from google.cloud.automl import enums

        logging.getLogger().setLevel(logging.INFO)  # TODO: make level configurable

        if api_endpoint:
          client_options = ClientOptions(api_endpoint=api_endpoint)
          client = automl.AutoMlClient(client_options=client_options)
        else:
          client = automl.AutoMlClient()

        try:
            logging.info('Deploying model {}'.format(model_display_name))
            model_full_id = client.model_path(gcp_project_id, gcp_region, model_id)
            response = client.deploy_model(model_full_id)
            # synchronous wait
            logging.info("Model deployed. {}".format(response.result()))
            status = 'deployed'
        except exceptions.NotFound as e:
          logging.warning(e)
          status = 'not_found'
        except Exception as e:
          logging.warning(e)
          status = 'undeployed'

        logging.info('Model status: {}'.format(status))
        return (model_display_name, model_id, status)

      def _serialize_str(str_value: str) -> str:
          if not isinstance(str_value, str):
              raise TypeError('Value "{}" has type "{}" instead of str.'.format(str(str_value), str(type(str_value))))
          return str_value

      import argparse
      _parser = argparse.ArgumentParser(prog='Automl deploy nlp model', description='')
      _parser.add_argument("--gcp-project-id", dest="gcp_project_id", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--gcp-region", dest="gcp_region", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--model-display-name", dest="model_display_name", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--model-id", dest="model_id", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--api-endpoint", dest="api_endpoint", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
      _parsed_args = vars(_parser.parse_args())
      _output_files = _parsed_args.pop("_output_paths", [])

      _outputs = automl_deploy_nlp_model(**_parsed_args)

      _output_serializers = [
          _serialize_str,
          _serialize_str,
          _serialize_str,

      ]

      import os
      for idx, output_file in enumerate(_output_files):
          try:
              os.makedirs(os.path.dirname(output_file))
          except OSError:
              pass
          with open(output_file, 'w') as f:
              f.write(_output_serializers[idx](_outputs[idx]))
    args:
    - --gcp-project-id
    - {inputValue: gcp_project_id}
    - --gcp-region
    - {inputValue: gcp_region}
    - --model-display-name
    - {inputValue: model_display_name}
    - --model-id
    - {inputValue: model_id}
    - if:
        cond: {isPresent: api_endpoint}
        then:
        - --api-endpoint
        - {inputValue: api_endpoint}
    - '----output-paths'
    - {outputPath: model_display_name}
    - {outputPath: model_id}
    - {outputPath: status}
